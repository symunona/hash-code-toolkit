const fs = require('fs');
const archiver = require('archiver');

module.exports = function (outputFileName, fileList, callback, cutPath) {
    
    // create a file to stream archive data to.
    var output = fs.createWriteStream(outputFileName);
    var archive = archiver('zip', {
        zlib: { level: 9 } // Sets the compression level.
    });

    // listen for all archive data to be written
    // 'close' event is fired only when a file descriptor is involved
    output.on('close', function () {
        console.log(archive.pointer() + ' total bytes');
        callback(archive.pointer() + ' total bytes');
    });

    // This event is fired when the data source is drained no matter what was the data source.
    // It is not part of this library but rather from the NodeJS Stream API.
    // @see: https://nodejs.org/api/stream.html#stream_event_end
    output.on('end', function () {
        console.log('Data has been drained');
    });

    // good practice to catch warnings (ie stat failures and other non-blocking errors)
    archive.on('warning', function (err) {
        console.warn('[archiver] warning', err)
        if (err.code === 'ENOENT') {
            // log warning
        } else {
            // throw error
            throw err;
        }
    });

    // good practice to catch this error explicitly
    archive.on('error', function (err) {
        throw err;
    });

    
    // append a file from stream
    var file1 = __dirname + '/file1.txt';
    
    fileList.map((fileName)=>archive.file(fileName, { 
        name: cutPath?fileName.substr(fileName.lastIndexOf('/')):fileName }));
    
    // pipe archive data to the file
    archive.pipe(output);

    // finalize the archive (ie we are done appending files but streams have to finish yet)
    // 'close', 'end' or 'finish' may be fired right after calling this method so register to them beforehand
    archive.finalize();
}